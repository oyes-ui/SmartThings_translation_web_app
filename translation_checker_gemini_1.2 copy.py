# -*- coding: utf-8 -*-
"""
translation_checker_gemini_1.0.py

# ì—…ë°ì´íŠ¸ ë‚´ì—­
# 0.2 ìš©ì–´ì§‘ ì¸ì‹ ìˆ˜ì •, GPT ê²°ì œë¬¸ì œë¡œ gpt ì œê±°
# 0.3 ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”
# 0.4 ë¹„ë™ê¸° ì²˜ë¦¬ì‹œ ìˆœì„œê°€ ë’¤ì£½ë°•ì£½ìœ¼ë¡œ ê¸°ì…ë˜ë˜ ê²ƒ ìˆ˜ì •
# 0.6 ìš©ì–´ì§‘ ë‚´ ê·œì¹™ ë‚´ìš©ë„ í”„ë¡¬í¬íŠ¸ì— ë°˜ì˜í•˜ë„ë¡ ìˆ˜ì •
# 0.7 ê²€ìˆ˜ í”„ë¡¬í¬íŠ¸ ëŒ€ë¬¸ì ê²€ìˆ˜ ê°•í™”, API Timeout 90ì´ˆ ì„¤ì •
# 0.8 ì‹œíŠ¸ ì´ë¦„(sheet_names) ì¸ìˆ˜ë¥¼ í†µí•œ ì„ íƒì  ê²€ìˆ˜ ê¸°ëŠ¥ ì¶”ê°€ ë° ë²„ê·¸ ìˆ˜ì • (FINAL)
# 0.8A ì„¸ë§ˆí¬ì–´(ë™ì‹œì„± ì œí•œ), ì§§ì€ í…ìŠ¤íŠ¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸, ìš©ì–´ì§‘ ì‚¬ì „ ë¶ˆì¼ì¹˜ ê°ì§€ ì¶”ê°€
# 0.9 ì‹œíŠ¸ë³„ ì–¸ì–´/ì½”ë“œ ë§¤í•‘(--sheet_langs / --sheet_langs_file) + ìš©ì–´ì§‘ ë‹¤ì–¸ì–´ ì»¬ëŸ¼ ì§€ì› + ë””ë²„ê·¸
# 1.0 ëŒ€ì†Œë¬¸ì(ë¬¸ì¥í˜•) í•˜ë“œë£° ê°•í™” + ìš©ì–´ì§‘ ì¼€ì´ìŠ¤ ê²€ìˆ˜ + LLM í”„ë¡¬í”„íŠ¸ì— ì¼€ì´ìŠ¤/ê³ ìœ ëª…/ê¸°ëŠ¥ëª… í‰ê°€Â·ìˆ˜ì •ì•ˆ ëª…ì‹œ

í•„ìˆ˜:
- pip install google-generativeai openpyxl python-dotenv

í™˜ê²½:
- .env íŒŒì¼ì— GEMINI_API_KEY=... ì„¤ì •
"""

import openpyxl
import csv
import os
import argparse
import google.generativeai as genai
from datetime import datetime
from dotenv import load_dotenv
import asyncio
import json
import re

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤)
load_dotenv()

# ----------------- ëŒ€ì†Œë¬¸ì í•˜ë“œë£° ì ìš© ëŒ€ìƒ ì–¸ì–´ -----------------
# sheet_langs.json ì—ì„œ ì˜¤ëŠ” lang ê°’ ê¸°ì¤€ (ì˜ˆ: "English", "German", "French" ...)
CASE_APPLICABLE_LANG_PREFIXES = {
    "English", "German", "French", "Spanish", "Portuguese",
    "Italian", "Dutch", "Swedish", "Polish", "Turkish",
    "Indonesian", "Vietnamese", "Russian"
}


def _is_case_sensitive_language(lang_name: str) -> bool:
    if not lang_name:
        return False
    return any(lang_name.startswith(pref) for pref in CASE_APPLICABLE_LANG_PREFIXES)


# ----------------- ê²½ë¡œ í•´ì„ ìœ í‹¸ (ìƒëŒ€/ì ˆëŒ€/í™ˆ/ìŠ¤í¬ë¦½íŠ¸ í´ë” íƒìƒ‰) -----------------
def _resolve_path(p):
    """
    ì£¼ì–´ì§„ ê²½ë¡œ pë¥¼ ë‹¤ìŒ ìˆœì„œë¡œ í•´ì„í•˜ì—¬ ì¡´ì¬í•˜ëŠ” ê²½ë¡œë¥¼ ë°˜í™˜:
    1) ì ˆëŒ€ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ
    2) ~ í™ˆ í™•ì¥
    3) í˜„ì¬ ì‘ì—… í´ë”(CWD) ê¸°ì¤€
    4) ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” í´ë” ê¸°ì¤€
    ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë§ˆì§€ë§‰ í›„ë³´(CWD ê¸°ì¤€) ê²½ë¡œë¥¼ ë°˜í™˜
    """
    if not p:
        return None
    p = os.path.expanduser(p)
    if os.path.isabs(p) and os.path.exists(p):
        return p
    # í›„ë³´êµ°
    candidates = [
        os.path.join(os.getcwd(), p),
        os.path.join(os.path.dirname(__file__), p),
    ]
    for c in candidates:
        if os.path.exists(c):
            return c
    # ë§ˆì§€ë§‰ìœ¼ë¡œ CWD ê¸°ì¤€ ê²½ë¡œë¥¼ ë°˜í™˜(íŒŒì¼ì´ ì—†ì–´ë„ ê²½ë¡œ ë¬¸ìì—´ì€ ë¦¬í„´)
    return os.path.join(os.getcwd(), p)


class TranslationChecker:
    """
    ì—‘ì…€ ë²ˆì—­ í’ˆì§ˆ ê²€ìˆ˜ê¸° (Gemini ë‹¨ì¼ ëª¨ë¸ ê¸°ë°˜)
    - ë¹„ë™ê¸° ì²˜ë¦¬ + ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´
    - ë¬´ì˜ë¯¸/ì§§ì€ í…ìŠ¤íŠ¸ ìŠ¤í‚µ + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì˜ˆì™¸
    - ìš©ì–´ì§‘ ì‚¬ì „ ë¶ˆì¼ì¹˜ ê°ì§€(ì‹œíŠ¸ë³„ íƒ€ê²Ÿ ì–¸ì–´ ì½”ë“œ ì‚¬ìš©)
    - ì‹œíŠ¸ë³„ ì–¸ì–´/ì–¸ì–´ì½”ë“œ ë§¤í•‘(--sheet_langs / --sheet_langs_file) ì§€ì›
    - ìš©ì–´ì§‘ CSVì˜ ë‹¤ì–¸ì–´ ì»¬ëŸ¼ì„ í•œ ë²ˆì— ë¡œë“œí•˜ì—¬ ì‹œíŠ¸ë³„ ì½”ë“œì— ë§ê²Œ ì‚¬ìš©
    - (1.0) íƒ€ê²Ÿ ì–¸ì–´ì— ëŒ€í•œ ëŒ€ì†Œë¬¸ì(ë¬¸ì¥í˜•) í•˜ë“œë£° & ìš©ì–´ì§‘ ì¼€ì´ìŠ¤ ì²´í¬
    """

    def __init__(
        self,
        gemini_api_key: str,
        model_name: str = "gemini-2.5-flash",
        max_concurrency: int = 10,
        short_text_whitelist=None,
        skip_llm_when_glossary_mismatch: bool = False,
        no_backtranslation = False
    ):
        if not gemini_api_key:
            raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '.env' íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # API ì„¤ì •
        genai.configure(api_key=gemini_api_key)
        self.model_name = model_name
        self.qa_model = genai.GenerativeModel(model_name)

        # â˜… ì¶”ê°€ëœ í”Œë˜ê·¸ ì €ì¥
        self.no_backtranslation = no_backtranslation

        # ë™ì‹œì„± ì œí•œ(ì„¸ë§ˆí¬ì–´)
        if max_concurrency < 1:
            max_concurrency = 1
        self.max_concurrency = max_concurrency
        self._sem = asyncio.Semaphore(self.max_concurrency)

        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ì§§ì•„ë„ ì¤‘ìš”í•œ ë¼ë²¨/ì•½ì–´ ì˜ˆì™¸)
        default_whitelist = {"ok", "on", "off", "ai", "5g", "go", "up", "usb", "nfc"}
        if short_text_whitelist:
            if isinstance(short_text_whitelist, str):
                extra = {x.strip().lower() for x in short_text_whitelist.split(",") if x.strip()}
            else:
                extra = {str(x).strip().lower() for x in short_text_whitelist}
            default_whitelist |= extra
        self.short_text_whitelist = default_whitelist

        # ìš©ì–´ì§‘ ì‚¬ì „ ë¶ˆì¼ì¹˜ ê°ì§€ ì‹œ LLM í˜¸ì¶œ ìƒëµ ì˜µì…˜
        self.skip_llm_when_glossary_mismatch = skip_llm_when_glossary_mismatch

        # ìš©ì–´ì§‘ êµ¬ì¡°:
        # {
        #   source_term: {
        #       'targets': { 'í•œêµ­ì–´': 'ìŠ¤ë§ˆíŠ¸ì‹±ìŠ¤', 'ì˜ì–´_ë¯¸êµ­': 'SmartThings', ... },
        #       'rule': '...'
        #   },
        #   ...
        # }
        self.glossary = {}
        self.glossary_headers = []  # ì „ì²´ í—¤ë” ìœ ì§€(ë””ë²„ê·¸ìš©)
        self.source_lang_code = None  # ë¡œë”© ì‹œ ì§€ì •
        self.rule_header = None       # 'ì„¤ëª…/ê·œì¹™' ë˜ëŠ” 'ê·œì¹™' ë˜ëŠ” ì—†ìŒ

    # ----------------- CSV Header Guard -----------------
    def _read_csv_with_header_guard(self, csv_path):
        """
        í—¤ë” ì´ì¤‘í–‰ ë°©ì§€:
        - 1í–‰ê³¼ 2í–‰ì´ ë™ì¼ í—¤ë”ë©´ ë‘˜ì§¸ í–‰ë¶€í„° ë°ì´í„°ë¡œ ê°„ì£¼
        """
        with open(csv_path, "r", encoding="utf-8-sig") as f:
            lines = f.readlines()
        if not lines:
            return csv.DictReader([])

        first = lines[0].strip()
        second = lines[1].strip() if len(lines) > 1 else None
        if second and first == second:
            return csv.DictReader(lines[1:])
        return csv.DictReader(lines)

    def load_glossary_multi(self, csv_path, source_lang_code: str):
        """
        ë‹¤ì–¸ì–´ ì»¬ëŸ¼ì„ ëª¨ë‘ ë¡œë“œ.
        - source_lang_code: ì›ë¬¸ì´ ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ëª…(ì˜ˆ: 'ì˜ì–´_ë¯¸êµ­' í˜¹ì€ 'en_US'ê°€ ì•„ë‹Œ CSV ì‹¤ì œ í—¤ë”ëª…)
        - ë‚˜ë¨¸ì§€ ëª¨ë“  ì–¸ì–´ ì»¬ëŸ¼ì„ targetsì— ìˆ˜ì§‘
        """
        try:
            reader = self._read_csv_with_header_guard(csv_path)
            headers = reader.fieldnames or []
            self.glossary_headers = headers[:]
            self.source_lang_code = source_lang_code
            self.rule_header = (
                "ì„¤ëª…/ê·œì¹™" if "ì„¤ëª…/ê·œì¹™" in headers else ("ê·œì¹™" if "ê·œì¹™" in headers else None)
            )

            if source_lang_code not in headers:
                print(f"âš  ê²½ê³ : ìš©ì–´ì§‘ì— ì›ë¬¸ ì»¬ëŸ¼ '{source_lang_code}'ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(headers)}")
                return

            count = 0
            for row in reader:
                source_term = (row.get(source_lang_code) or "").strip()
                if not source_term:
                    continue

                # rule
                rule = (row.get(self.rule_header) or "").strip() if self.rule_header else ""

                # targets: ëª¨ë“  ì»¬ëŸ¼(ì›ë¬¸/ê·œì¹™ ì œì™¸)ì„ íƒ€ê²Ÿ í›„ë³´ë¡œ ìˆ˜ì§‘
                targets = {}
                for col in headers:
                    if col == source_lang_code:
                        continue
                    if self.rule_header and col == self.rule_header:
                        continue
                    val = (row.get(col) or "").strip()
                    if val:
                        targets[col] = val

                if targets:
                    self.glossary[source_term] = {"targets": targets, "rule": rule}
                    count += 1

            print(f"âœ“ ìš©ì–´ì§‘ ë¡œë“œ ì™„ë£Œ: {count}ê°œ í•­ëª© (ë‹¤ì–¸ì–´ íƒ€ê²Ÿ í¬í•¨)")

        except FileNotFoundError:
            print(f"âš  ìš©ì–´ì§‘ íŒŒì¼ '{csv_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            print(f"âš  ìš©ì–´ì§‘ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ----------------- ì—‘ì…€ ë¡œë”© -----------------
    def load_excel_files(self, source_file, target_file, cell_range, selected_sheets=None):
        """ì—‘ì…€ íŒŒì¼ì—ì„œ ì§€ì •ëœ ì…€ ë²”ìœ„ ë°ì´í„°ë¥¼ 'ì‹œíŠ¸ ìˆœì„œ â†’ ì…€ ìˆœì„œ'ë¡œ í‰íƒ„í™”í•´ì„œ ë°˜í™˜"""
        all_data = []

        try:
            wb_source = openpyxl.load_workbook(source_file)
            wb_target = openpyxl.load_workbook(target_file)
        except FileNotFoundError as e:
            print(f"âš  ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

        # ì—‘ì…€ ì›ë³¸ ì‹œíŠ¸ ìˆœì„œ ìœ ì§€
        source_order = wb_source.sheetnames
        target_set = set(wb_target.sheetnames)

        # ê³µí†µ ì‹œíŠ¸ë¥¼ 'ì›ë³¸ ìˆœì„œ'ë¡œë§Œ ì„ ë³„
        common_sheets_in_order = [s for s in source_order if s in target_set]
        if not common_sheets_in_order:
            print("âš  ê²½ê³ : ì›ë¬¸ íŒŒì¼ê³¼ ë²ˆì—­ë³¸ íŒŒì¼ ì‚¬ì´ì— ì´ë¦„ì´ ì¼ì¹˜í•˜ëŠ” ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return []

        # ìµœì¢… ëŒ€ìƒ ì‹œíŠ¸: ì‚¬ìš©ìê°€ ì§€ì •í–ˆìœ¼ë©´ ê·¸ 'ì§€ì • ìˆœì„œ' ìœ ì§€, ì•„ë‹ˆë©´ ê³µí†µ ì „ì²´
        if selected_sheets:
            # ì…ë ¥ëœ ìˆœì„œ ê·¸ëŒ€ë¡œ, ì‹¤ì œ ê³µí†µ ì‹œíŠ¸ì— ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ
            target_sheets_to_process = [s for s in selected_sheets if s in common_sheets_in_order]
            if not target_sheets_to_process:
                print(f"âš  ê²½ê³ : ì§€ì •ëœ ì‹œíŠ¸({', '.join(selected_sheets)}) ì¤‘ ê³µí†µ íŒŒì¼ì— ì¡´ì¬í•˜ëŠ” ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
        else:
            target_sheets_to_process = common_sheets_in_order

        print(f"\nâœ“ ê²€ìˆ˜ ëŒ€ìƒ ì‹œíŠ¸: {', '.join(target_sheets_to_process)}")

        # ì‹œíŠ¸ ìˆœì„œ ìœ ì§€ + ì…€ ìˆœì„œ(í–‰â†’ì—´)ëŒ€ë¡œ í‰íƒ„í™”
        for sheet_name in target_sheets_to_process:
            ws_source = wb_source[sheet_name]
            ws_target = wb_target[sheet_name]
            extracted = 0
            try:
                for source_row, target_row in zip(ws_source[cell_range], ws_target[cell_range]):
                    for s_cell, t_cell in zip(source_row, target_row):
                        s_val = str(s_cell.value).strip() if s_cell.value is not None else ""
                        t_val = str(t_cell.value).strip() if t_cell.value is not None else ""
                        if s_val and t_val:
                            all_data.append({
                                "cell_ref": s_cell.coordinate,
                                "sheet_name": sheet_name,
                                "source": s_val,
                                "target": t_val,
                            })
                            extracted += 1
            except Exception as e:
                print(f"  âš  '{sheet_name}' ì‹œíŠ¸ '{cell_range}' ë²”ìœ„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

            if extracted:
                print(f"  âœ“ '{sheet_name}': {extracted}ê°œ í•­ëª© ì¶”ì¶œ ì™„ë£Œ")

        return all_data  # ì´ë¯¸ 'ì‹œíŠ¸ ìˆœì„œ â†’ ì…€ ìˆœì„œ'ë¡œ ì •ë ¬ëœ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸

    # ----------------- ìœ í‹¸ -----------------
    async def _with_semaphore(self, coro):
        async with self._sem:
            return await coro

    def _get_target_term_for_code(self, source_term: str, target_lang_code: str):
        """í•´ë‹¹ source_termì— ëŒ€í•´ íƒ€ê²Ÿ ì–¸ì–´ ì½”ë“œì˜ ìš©ì–´ë¥¼ ëŒë ¤ì¤ë‹ˆë‹¤. ì—†ìœ¼ë©´ None."""
        entry = self.glossary.get(source_term)
        if not entry:
            return None
        return entry["targets"].get(target_lang_code)

    def _build_glossary_lines_for_code(self, target_lang_code: str):
        """í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ìš©ì–´ì§‘ ë¼ì¸ ìƒì„±(í˜„ì¬ íƒ€ê²Ÿ ì–¸ì–´ ì½”ë“œ ì „ìš©)."""
        if not self.glossary:
            return "ìš©ì–´ì§‘ ì—†ìŒ"

        out = []
        for source_term, meta in self.glossary.items():
            tgt = meta["targets"].get(target_lang_code)
            if not tgt:
                continue
            rule = meta.get("rule")
            rule_info = f" (ê·œì¹™: {rule})" if rule else ""
            out.append(f"- ì›ì–´: {source_term} â†’ ëŒ€ìƒì–´({target_lang_code}): {tgt}{rule_info}")
        return "\n".join(out) if out else f"ìš©ì–´ì§‘ì— '{target_lang_code}' íƒ€ê²Ÿ í•­ëª© ì—†ìŒ"

    # ----------------- ì‚¬ì „ ë¶ˆì¼ì¹˜ ê°ì§€ -----------------
    def _precheck_glossary_mismatch(self, source_text: str, target_text: str, target_lang_code: str):
        """
        ë§¤ìš° ê°„ë‹¨í•œ ì‚¬ì „ ê°ì§€:
        - ì›ë¬¸ì— source_termì´ í¬í•¨ë˜ë©´, ë²ˆì—­ë¬¸ì— í•´ë‹¹ íƒ€ê²Ÿì½”ë“œì˜ target_termì´ í¬í•¨ë˜ëŠ”ì§€ë§Œ í™•ì¸(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        """
        if not self.glossary or not target_lang_code:
            return []

        mismatches = []
        src_lower = source_text.lower()
        tgt_lower = target_text.lower()

        for s_term, meta in self.glossary.items():
            t_term = meta["targets"].get(target_lang_code)
            if not t_term:
                continue  # í•´ë‹¹ ì½”ë“œì— ëŒ€í•œ íƒ€ê²Ÿ ë¯¸ì •ì˜
            if s_term and s_term.lower() in src_lower:
                if t_term.lower() not in tgt_lower:
                    mismatches.append(f"'{s_term}' â†’ '{t_term}' ë¯¸ì ìš©({target_lang_code})")
        return mismatches

    # ----------------- ìš©ì–´ì§‘ "ëŒ€ì†Œë¬¸ì" ì²´í¬ -----------------
    def _check_glossary_casing(self, source_text: str, target_text: str, target_lang_code: str):
        """
        - ì›ë¬¸ì— source_termì´ ë‚˜ì˜¤ê³ 
        - ë²ˆì—­ë¬¸ì— í•´ë‹¹ target_lang_codeì˜ ìš©ì–´ê°€ í¬í•¨ë˜ì–´ ìˆì„ ë•Œ
        - ì‹¤ì œ ë²ˆì—­ë¬¸ ì•ˆì˜ í‘œê¸°ê°€ ìš©ì–´ì§‘ í‘œê¸°ì™€ 'ì² ìëŠ” ê°™ì§€ë§Œ ì¼€ì´ìŠ¤ë§Œ ë‹¤ë¥¸ì§€' í™•ì¸
        """
        if not self.glossary or not target_lang_code:
            return []

        if not source_text or not target_text:
            return []

        issues = []
        src_lower = source_text.lower()
        tgt_lower = target_text.lower()

        for s_term, meta in self.glossary.items():
            t_term = meta["targets"].get(target_lang_code)
            if not t_term:
                continue

            if s_term.lower() in src_lower and t_term.lower() in tgt_lower:
                idx = tgt_lower.find(t_term.lower())
                if idx == -1:
                    continue
                actual = target_text[idx:idx + len(t_term)]
                # ì² ìëŠ” ê°™ì€ë° ì¼€ì´ìŠ¤ë§Œ ë‹¤ë¥´ë©´ ì´ìŠˆë¡œ ë³´ê³ 
                if actual.lower() == t_term.lower() and actual != t_term:
                    issues.append(f"ìš©ì–´ì§‘ '{t_term}'ì˜ ëŒ€ì†Œë¬¸ì í‘œê¸°ê°€ '{actual}'ë¡œ ì‚¬ìš©ë¨")

        return issues

    # ----------------- ëŒ€ì†Œë¬¸ì í•˜ë“œë£°(ë¬¸ì¥í˜•) ë¶„ì„ -----------------
    def _analyze_sentence_case(self, target_text: str, target_lang: str):
        """
        - íƒ€ê²Ÿ ì–¸ì–´ê°€ CASE_APPLICABLE_LANG_PREFIXES ì— í•´ë‹¹í•˜ë©´
        - ë¬¸ì¥ì„ .!? ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê³ 
        - ê° ë¬¸ì¥ì—ì„œ 'ì²« ì•ŒíŒŒë²³ë§Œ ëŒ€ë¬¸ì + ë‚˜ë¨¸ì§€ ì†Œë¬¸ì' ì—¬ë¶€ë¥¼ ë‹¨ìˆœ íŒì •
        - ì „ì²´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ 'ë‹¨ìˆœ ë£° ê¸°ë°˜ ë¬¸ì¥í˜• ë³€í™˜ì•ˆ'ë„ í•¨ê»˜ ë°˜í™˜ (ì°¸ê³ ìš©)
        - âš  ì•ŒíŒŒë²³ íŒì •ì€ ìœ ë‹ˆì½”ë“œ ê¸°ë°˜(str.isalpha/isupper/islower)ìœ¼ë¡œ,
          ë¼í‹´Â·í‚¤ë¦´Â·ë² íŠ¸ë‚¨ì–´ ë“± ì¼€ì´ìŠ¤ ìˆëŠ” ë¬¸ì ì „ë¶€ ì§€ì›
        """
        if not target_text:
            return None, None
        if not _is_case_sensitive_language(target_lang):
            # í•œêµ­ì–´/ì¤‘êµ­ì–´/ì¼ë³¸ì–´ ë“± ì¼€ì´ìŠ¤ ê°œë… ì—†ëŠ” ì–¸ì–´ëŠ” ìŠ¤í‚µ
            return None, None

        text = target_text.strip()
        # ë¬¸ì¥ ë¶„ë¦¬: ., !, ? ë’¤ì˜ ê³µë°± ê¸°ì¤€ (ì•„ì£¼ ë‹¨ìˆœí•œ ê¸°ì¤€)
        sentences = re.split(r'(?<=[\.!?])\s+', text)
        sentences = [s for s in sentences if s.strip()]
        if not sentences:
            return None, None

        report_lines = []
        fixed_sentences = []

        for idx, sent in enumerate(sentences, start=1):
            s = sent

            # âœ… ìœ ë‹ˆì½”ë“œ ê¸°ë°˜: ì²˜ìŒ ë‚˜ì˜¤ëŠ” "ê¸€ì"(isalpha=True)ë¥¼ ì°¾ìŒ
            first_alpha_index = None
            for i, ch in enumerate(s):
                if ch.isalpha():  # ë¼í‹´, í‚¤ë¦´, ë² íŠ¸ë‚¨ì–´ ë“± ëª¨ë‘ í¬í•¨
                    first_alpha_index = i
                    break

            if first_alpha_index is None:
                # ì•ŒíŒŒë²³/ê¸€ì ìì²´ê°€ ì—†ìœ¼ë©´ ì¼€ì´ìŠ¤ íŒì • ë¶ˆí•„ìš”
                report_lines.append(f"- ë¬¸ì¥ {idx}: ì•ŒíŒŒë²³/ë¬¸ì ì—†ìŒ â†’ ëŒ€ì†Œë¬¸ì íŒì • ìƒëµ")
                fixed_sentences.append(s)
                continue

            first_char = s[first_alpha_index]
            rest = s[first_alpha_index + 1 :]

            # ìœ ë‹ˆì½”ë“œ ê¸°ë°˜ ì†Œë¬¸ì/ëŒ€ë¬¸ì íŒì •
            is_sentence_case = first_char.isupper() and rest == rest.lower()

            # ì²« ê¸€ì ì´í›„ì˜ 'ì¶”ê°€ ëŒ€ë¬¸ì' ê°¯ìˆ˜
            extra_caps = sum(
                1 for ch in rest
                if ch.isalpha() and ch.isupper()
            )

            status = "ë¬¸ì¥í˜•(ì²« ê¸€ìë§Œ ëŒ€ë¬¸ì)" if is_sentence_case else "ë¬¸ì¥í˜• ì•„ë‹˜"
            report_lines.append(
                f"- ë¬¸ì¥ {idx}: {status}, ì¶”ê°€ ëŒ€ë¬¸ì ìˆ˜: {extra_caps}ê°œ"
            )

            # ğŸ”§ ë‹¨ìˆœ ë£° ê¸°ë°˜ ë³€í™˜ì•ˆ:
            #   - ì²« ê¸€ìëŠ” ëŒ€ë¬¸ì ìœ ì§€
            #   - ë‚˜ë¨¸ì§€ ì•ŒíŒŒë²³ì€ ì „ë¶€ ì†Œë¬¸ìë¡œ ë³€í™˜
            fixed_rest = rest.lower()
            fixed_sent = s[:first_alpha_index] + first_char.upper() + fixed_rest
            fixed_sentences.append(fixed_sent)

        simple_fixed_text = " ".join(fixed_sentences)
        report = "\n".join(report_lines)

        # ì›ë¬¸ê³¼ ì™„ì „íˆ ê°™ìœ¼ë©´ êµ³ì´ ì œì•ˆ ì•ˆ í•¨
        if simple_fixed_text == target_text:
            simple_fixed_text = None

        return report, simple_fixed_text

    # ----------------- LLM í˜¸ì¶œ -----------------
    async def check_with_gemini_qa(
        self,
        source_text,
        target_text,
        source_lang,
        target_lang,
        target_lang_code: str,
        max_retries=3,
    ):
        """Gemini ìƒì„¸ ê²€ìˆ˜(í•œêµ­ì–´ ê²°ê³¼). íƒ€ê²Ÿ ì–¸ì–´ ì½”ë“œ ê¸°ë°˜ ìš©ì–´ì§‘ ë¼ì¸ë§Œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨."""
        glossary_text = self._build_glossary_lines_for_code(target_lang_code)

        prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ ê²€ìˆ˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[Context]
- ì›ë¬¸({source_lang}): {source_text}
- ë²ˆì—­ë¬¸({target_lang}/{target_lang_code}): {target_text}

[ìš©ì–´ì§‘({target_lang_code})]
{glossary_text}

ë‹¤ìŒ í•­ëª©ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ê°ê´€ì ì¸ ê²€ìˆ˜ ê²°ê³¼ë¥¼ **í•œêµ­ì–´ ë¶ˆë › í¬ì¸íŠ¸**ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”:
1. **ë¬¸ë²•/ìœ ì°½ì„±**: ë²ˆì—­ë¬¸ì˜ ì–´ìƒ‰í•œ í‘œí˜„ì´ë‚˜ ë¬¸ë²• ì˜¤ë¥˜.
2. **ë¬¸í™”/ë¬¸ë§¥ ì ì ˆì„±**: ë‰˜ì•™ìŠ¤ ì†ì‹¤, ë¬¸í™”ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ ìš”ì†Œ. **(ì°¸ê³ : 'German for Casual Audience (Du-form mandatory)' ë“± ì–¸ì–´ ì§€ì •ì´ ìˆëŠ” ê²½ìš°, í˜¸ì¹­(Du/Sie) ì¼ê´€ì„± í™•ì¸)**
3. **ëŒ€ì†Œë¬¸ì(Casing) ë° ë¬¸ì¥í˜•**:
   - ê° ë¬¸ì¥ì—ì„œ ì²« ë‹¨ì–´ë§Œ ëŒ€ë¬¸ìì´ê³  ë‚˜ë¨¸ì§€ëŠ” ì†Œë¬¸ìì¸ì§€(ë¬¸ì¥í˜•) í‰ê°€í•˜ì„¸ìš”.
   - ë¬¸ì¥ ì¤‘ê°„ì— ë“±ì¥í•˜ëŠ” ëŒ€ë¬¸ìÂ·ALL CAPS ë‹¨ì–´ê°€ ê³ ìœ ëª…/ë¸Œëœë“œëª…/ê¸°ëŠ¥ëª…/ì•½ì–´ì¸ì§€ ì—¬ë¶€ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
4. **ìš©ì–´ì§‘ ë° ê·œì¹™ ì¤€ìˆ˜**:
   - ìš©ì–´ì§‘ì— ìˆëŠ” ìš©ì–´ê°€ ì˜¬ë°”ë¥´ê²Œ ë²ˆì—­ë˜ì—ˆëŠ”ì§€ë¿ë§Œ ì•„ë‹ˆë¼,
   - ìš©ì–´ì§‘ì— ëª…ì‹œëœ ëŒ€ì†Œë¬¸ì í‘œê¸°(SmartThings, Galaxy Watch ë“±)ê°€ ê·¸ëŒ€ë¡œ ì§€ì¼œì¡ŒëŠ”ì§€ë„ í‰ê°€í•˜ì„¸ìš”.
5. **ìˆ˜ì • ì œì•ˆ**:
   - ë¬¸ì œê°€ ìˆë‹¤ê³  íŒë‹¨ë˜ëŠ” ë¶€ë¶„ì´ ìˆì„ ê²½ìš°,
   - ë¬¸ì¥í˜•(ì²« ê¸€ìë§Œ ëŒ€ë¬¸ì) ê¸°ì¤€ì„ í•´ì¹˜ì§€ ì•Šìœ¼ë©´ì„œ
   - ê³ ìœ ëª…/ê¸°ëŠ¥ëª…/ì•½ì–´/ìš©ì–´ì§‘ í‘œê¸°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ëŠ” **ì•ˆì „í•œ ìˆ˜ì •ì•ˆ**ì„ ì œì‹œí•˜ì„¸ìš”.

ë¬¸ì œê°€ ì—†ë‹¤ë©´ ë§ˆì§€ë§‰ ì¤„ì—:
"ìµœì¢… í‰ê°€: ìš°ìˆ˜, ì£¼ìš” ë¬¸ì œ ì—†ìŒ."
"""

        for attempt in range(max_retries):
            try:
                response = await self.qa_model.generate_content_async(
                    prompt,
                    generation_config={"temperature": 0.2},
                    request_options={"timeout": 90},
                )
                text = getattr(response, "text", None)
                return text.strip() if text else "[ì‘ë‹µ ë¹„ì–´ìˆìŒ]"
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # 1,2,4ì´ˆ ë°±ì˜¤í”„
                else:
                    return f"[Gemini QA ì˜¤ë¥˜] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}"
        return "[Gemini QA ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"

    async def get_back_translation(self, target_text, target_lang, source_lang, max_retries=3):
        """Gemini ì—­ë²ˆì—­(ì„¤ëª… ì—†ì´ ë²ˆì—­ë¬¸ë§Œ)"""
        prompt = (
            f"ë‹¤ìŒ {target_lang} í…ìŠ¤íŠ¸ë¥¼ {source_lang}ìœ¼ë¡œ ë‹¤ì‹œ ë²ˆì—­í•´ì£¼ì„¸ìš”. "
            f"ì˜¤ì§ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n\n{target_text}"
        )
        for attempt in range(max_retries):
            try:
                response = await self.qa_model.generate_content_async(
                    prompt,
                    generation_config={"temperature": 0.1},
                    request_options={"timeout": 90},
                )
                text = getattr(response, "text", None)
                return text.strip() if text else "[ì‘ë‹µ ë¹„ì–´ìˆìŒ]"
            except Exception as e:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return f"[Gemini ì—­ë²ˆì—­ ì˜¤ë¥˜] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}"
        return "[Gemini ì—­ë²ˆì—­ ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"

    # ----------------- ë©”ì¸ ì²˜ë¦¬ -----------------
    async def process_translation_pair_async(
        self,
        data_pair,
        source_lang,
        default_target_lang,
        sheet_lang_map,  # dict: {sheet: {'lang': 'Korean', 'code': 'í•œêµ­ì–´'}}
        default_target_lang_code,
    ):
        """ë‹¨ì¼ ë²ˆì—­ ìŒ ì²˜ë¦¬ ë° ê²°ê³¼ ë¬¸ìì—´ ë°˜í™˜"""
        cell_ref = data_pair["cell_ref"]
        sheet_name = data_pair["sheet_name"]
        source = data_pair["source"]
        target = data_pair["target"]

        # ì‹œíŠ¸ë³„ ì–¸ì–´/ì½”ë“œ ê²°ì •
        tgt_lang = sheet_lang_map.get(sheet_name, {}).get("lang", default_target_lang)
        tgt_code = sheet_lang_map.get(sheet_name, {}).get("code", default_target_lang_code)

        # DEBUG: ì‹¤ì œ ì ìš©ëœ íƒ€ê²Ÿ ì–¸ì–´/ì½”ë“œ ì¶œë ¥
        print(f"[{sheet_name}] ì ìš© íƒ€ê²Ÿ = {tgt_lang} / {tgt_code}")

        # --- ìŠ¤í‚µ ë¡œì§ + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì˜ˆì™¸ ---
        is_placeholder = (
            (len(source) <= 2 and len(target) <= 2 and source.lower() == target.lower())
            or (not source.strip() and not target.strip())
            or (source.strip().lower() == target.strip().lower() and len(source.strip()) < 10)
        )
        if source.strip().lower() in self.short_text_whitelist or target.strip().lower() in self.short_text_whitelist:
            is_placeholder = False

        # --- ìš©ì–´ì§‘ ì‚¬ì „ ë¶ˆì¼ì¹˜ ê°ì§€(ì‹œíŠ¸ íƒ€ê²Ÿ ì½”ë“œ ê¸°ì¤€) ---
        pre_mismatch = self._precheck_glossary_mismatch(source, target, tgt_code)
        skip_llm = self.skip_llm_when_glossary_mismatch and bool(pre_mismatch)

        # --- ëŒ€ì†Œë¬¸ì í•˜ë“œë£°(ë¬¸ì¥í˜•) ë¶„ì„ & ìš©ì–´ì§‘ ì¼€ì´ìŠ¤ ì²´í¬ (Python ë ˆë²¨) ---
        case_report, simple_case_fix = self._analyze_sentence_case(target, tgt_lang)
        glossary_case_issues = self._check_glossary_casing(source, target, tgt_code)

        # ëŒ€ì†Œë¬¸ì ì ê²€ ì„¹ì…˜ í…ìŠ¤íŠ¸ êµ¬ì„±
        if case_report:
            case_section = "ëŒ€ì†Œë¬¸ì í•˜ë“œë£°(ë¬¸ì¥í˜•) ì ê²€:\n" + case_report
            if simple_case_fix:
                case_section += "\n\n[ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ë¬¸ì¥í˜• ë³€í™˜ì•ˆ(ì°¸ê³ ìš©)]:\n" + simple_case_fix
        else:
            case_section = "ë³„ë„ ì§€ì  ì‚¬í•­ ì—†ìŒ."

        # ìš©ì–´ì§‘ ì ê²€ ì„¹ì…˜ í…ìŠ¤íŠ¸ êµ¬ì„± (ì‚¬ì „ ê°ì§€ + ì¼€ì´ìŠ¤ ì´ìŠˆ ë¬¶ì–´ì„œ)
        glossary_parts = []
        if pre_mismatch:
            glossary_parts.append("ìš©ì–´ì§‘ ì‚¬ì „ ê°ì§€:\n- " + "\n- ".join(pre_mismatch))
        if glossary_case_issues:
            glossary_parts.append(
                "ìš©ì–´ì§‘ ëŒ€ì†Œë¬¸ì í‘œê¸° ì ê²€:\n" +
                "\n".join(f"- {msg}" for msg in glossary_case_issues)
            )
        if glossary_parts:
            glossary_section = "\n\n".join(glossary_parts)
        else:
            glossary_section = "ë³„ë„ ì§€ì  ì‚¬í•­ ì—†ìŒ."

        # --- placeholder (ì§§ì€/ë¬´ì˜ë¯¸ í…ìŠ¤íŠ¸) ì²˜ë¦¬ ---
        if is_placeholder and not pre_mismatch:
            back_translation = "[ê±´ë„ˆëœ€: í…ìŠ¤íŠ¸ê°€ ì§§ê±°ë‚˜ ë¬´ì˜ë¯¸í•˜ì—¬ AI í˜¸ì¶œì„ ìƒëµí–ˆìŠµë‹ˆë‹¤.]"
            gemini_qa_review = "[ê±´ë„ˆëœ€: í…ìŠ¤íŠ¸ê°€ ì§§ê±°ë‚˜ ë¬´ì˜ë¯¸í•˜ì—¬ AI í˜¸ì¶œì„ ìƒëµí–ˆìŠµë‹ˆë‹¤.]"

            result_content = (
                f"\n\n{'='*90}\n"
                f"[ì‹œíŠ¸] {sheet_name} | [ì…€] {cell_ref}\n"
                f"{'-'*90}\n\n"
                f"[ìƒì„¸ - ì›ë¬¸]\n"
                f"{source}\n\n"
                f"[ìƒì„¸ - ë²ˆì—­ë¬¸]\n"
                f"{target}\n\n"
                f"[ìƒì„¸ - ëŒ€ì†Œë¬¸ì ì ê²€]\n"
                f"{case_section}\n\n"
                f"[ìƒì„¸ - ìš©ì–´ì§‘ ì ê²€]\n"
                f"{glossary_section}\n\n"
                f"[ìƒì„¸ - ì—­ë²ˆì—­]\n"
                f"{back_translation}\n\n"
                f"[ìƒì„¸ - Gemini ê²€ìˆ˜ ê²°ê³¼]\n"
                f"{gemini_qa_review}\n"
                f"{'='*90}\n"
            )
            return result_content

        # --- LLM í˜¸ì¶œ ---
        print(f"[{sheet_name}] {cell_ref} ë¹„ë™ê¸° ì‘ì—… ì‹œì‘...")

        if skip_llm:
            back_translation = "[ì‚¬ì „ ê°ì§€ë¡œ LLM í˜¸ì¶œ ìƒëµ]"
            gemini_qa_review = "â€» ìš©ì–´ì§‘ ì‚¬ì „ ê°ì§€ ê²°ê³¼ë¥¼ ìš°ì„  ê²€í† í•˜ì„¸ìš”. (ì˜µì…˜ì— ì˜í•´ LLM í˜¸ì¶œ ìƒëµë¨)"
        else:
            qa_task = self._with_semaphore(
                self.check_with_gemini_qa(source, target, source_lang, tgt_lang, tgt_code)
            )

            # no_backtranslation ì˜µì…˜ì´ ì¼œì ¸ ìˆìœ¼ë©´ ì—­ë²ˆì—­ ìŠ¤í‚µ
            if getattr(self, "no_backtranslation", False):
                gemini_qa_review = await qa_task
                back_translation = "[ì—­ë²ˆì—­ ë¹„í™œì„±í™”ë¨ (--no_backtranslation)]"
            else:
                bt_task = self._with_semaphore(
                    self.get_back_translation(target, tgt_lang, source_lang)
                )
                gemini_qa_review, back_translation = await asyncio.gather(qa_task, bt_task)

        print(f"[{sheet_name}] {cell_ref} ì²˜ë¦¬ ì™„ë£Œ.")

        # ê²°ê³¼ ë¸”ë¡ ìµœì¢… ì¡°ë¦½
        result_content = (
            f"\n\n{'='*90}\n"
            f"[ì‹œíŠ¸] {sheet_name} | [ì…€] {cell_ref}\n"
            f"{'-'*90}\n\n"
            f"[ìƒì„¸ - ì›ë¬¸]\n"
            f"{source}\n\n"
            f"[ìƒì„¸ - ë²ˆì—­ë¬¸]\n"
            f"{target}\n\n"
            f"[ìƒì„¸ - ëŒ€ì†Œë¬¸ì ì ê²€]\n"
            f"{case_section}\n\n"
            f"[ìƒì„¸ - ìš©ì–´ì§‘ ì ê²€]\n"
            f"{glossary_section}\n\n"
            f"[ìƒì„¸ - ì—­ë²ˆì—­]\n"
            f"{back_translation}\n\n"
            f"[ìƒì„¸ - Gemini ê²€ìˆ˜ ê²°ê³¼]\n"
            f"{gemini_qa_review}\n"
            f"{'='*90}\n"
        )
        return result_content

    async def main_async(
        self,
        source_file,
        target_file,
        cell_range,
        source_lang,
        target_lang,
        source_lang_code=None,      # ì›ë¬¸ ì»¬ëŸ¼
        target_lang_code=None,      # (ê¸°ë³¸) íƒ€ê²Ÿ ì»¬ëŸ¼
        glossary_file=None,
        sheet_names=None,
        sheet_langs=None,           # "Sheet:LangName:LangHeader"
        sheet_langs_file=None,      # JSON ì¸ì
    ):
        """ë©”ì¸ ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜"""
        print("=" * 90)
        print("ë²ˆì—­ ê²€ìˆ˜ ìŠ¤í¬ë¦½íŠ¸ ver1.0 (Casing ê°•í™” / Semaphore + Whitelist + Precheck + SheetLangs)")
        print(f"ì‚¬ìš© ëª¨ë¸: {self.model_name}")
        print("=" * 90)

        # 1) ì‹œíŠ¸ë³„ ì–¸ì–´ ë§¤í•‘
        sheet_lang_map = {}

        # â˜… DEBUG: ë„˜ê²¨ë°›ì€ JSON ê²½ë¡œ/ì¡´ì¬ ì—¬ë¶€ + CWD/í•´ì„ê²½ë¡œ ì¶œë ¥
        print(f"[DEBUG] sheet_langs_file ì¸ìê°’ = {sheet_langs_file}")
        print(f"[DEBUG] CWD = {os.getcwd()}")
        resolved = _resolve_path(sheet_langs_file) if sheet_langs_file else None
        print(f"[DEBUG] resolved path = {resolved}")
        if resolved:
            print(f"[DEBUG] os.path.exists(resolved) = {os.path.exists(resolved)}")

        # 1-1) JSON íŒŒì¼ ìš°ì„ 
        if resolved and os.path.exists(resolved):
            try:
                with open(resolved, 'r', encoding='utf-8') as f:
                    loaded = json.load(f)
                # ê¸°ëŒ€ ìŠ¤í‚¤ë§ˆ: { "KR(í•œêµ­)": {"lang":"Korean","code":"í•œêµ­ì–´"}, ... }
                # í‚¤/ê°’ ì •ê·œí™”(ì–‘ë ê³µë°± ì œê±°)
                for k, v in loaded.items():
                    if not isinstance(v, dict):
                        print(f"âš  ë¬´ì‹œë¨: '{k}' ê°’ì´ ê°ì²´ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                        continue
                    name = str(k).strip()
                    lang = str(v.get("lang", "")).strip()
                    code = str(v.get("code", "")).strip()
                    if not name or not lang or not code:
                        print(f"âš  ë¬´ì‹œë¨: '{k}' ë§¤í•‘ì— lang/codeê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
                        continue
                    sheet_lang_map[name] = {"lang": lang, "code": code}
                print(f"âœ“ ì‹œíŠ¸ ì–¸ì–´ ë§¤í•‘(JSON) ë¡œë“œ: {len(sheet_lang_map)}ê°œ")
            except Exception as e:
                print(f"âš  sheet_langs.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            if sheet_langs_file:
                print(f"âš  ê²½ê³ : ì§€ì •ëœ JSON ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ â†’ {sheet_langs_file} (resolved: {resolved})")

        # 1-2) ë¬¸ìì—´ ì¸ì íŒŒì‹±(í´ë°±)
        if not sheet_lang_map and sheet_langs:
            pairs = [s.strip() for s in sheet_langs.split(",") if ":" in s]
            for p in pairs:
                try:
                    name, lang_name, lang_code = [x.strip() for x in p.split(":")]
                    if name and lang_name and lang_code:
                        sheet_lang_map[name] = {"lang": lang_name, "code": lang_code}
                except ValueError:
                    print(f"âš  ì‹œíŠ¸ ì–¸ì–´ ë§¤í•‘ êµ¬ë¬¸ ì˜¤ë¥˜: {p}")

        # 2) ìš©ì–´ì§‘ ë¡œë“œ(ë‹¤ì–¸ì–´)
        if glossary_file and os.path.exists(glossary_file):
            if source_lang_code:
                self.load_glossary_multi(glossary_file, source_lang_code)
            else:
                print("âš  ê²½ê³ : ìš©ì–´ì§‘ ì‚¬ìš© ì‹œ ì›ë¬¸ ì–¸ì–´ ì½”ë“œ(--src_code)ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            if glossary_file:
                print(f"âš  ìš©ì–´ì§‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {glossary_file}")

        # 3) ì—‘ì…€ ë¡œë“œ
        print(f"\nâ–¶ ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì¤‘...")
        selected_sheets_list = [s.strip() for s in sheet_names.split(",")] if sheet_names else None
        all_data = self.load_excel_files(
            source_file, target_file, cell_range, selected_sheets=selected_sheets_list
        )
        if not all_data:
            print("\nâš  ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ íŒŒì¼ ë§¤ì¹­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        total_items = len(all_data)
        print(f"\nâœ“ ë¡œë“œ ì™„ë£Œ: ì´ {total_items}ê°œ í•­ëª©")

        # (ì„ íƒ) ë§¤í•‘ ì§„ë‹¨: ì‹¤ì œ ì‹œíŠ¸ì™€ ë§¤í•‘ ëŒ€ì¡°
        actual_sheets = {item["sheet_name"] for item in all_data}
        if sheet_lang_map:
            missing = [name for name in sheet_lang_map if name not in actual_sheets]
            if missing:
                print(f"âš  ê²½ê³ : JSON/ë¬¸ìì—´ì—ë§Œ ìˆê³  ì—‘ì…€ì— ì—†ëŠ” ì‹œíŠ¸ â†’ {', '.join(missing)}")
            unmapped = [s for s in sorted(actual_sheets) if s not in sheet_lang_map]
            if unmapped:
                print(f"âš  ê²½ê³ : ë§¤í•‘ ì—†ëŠ” ì‹œíŠ¸ëŠ” ê¸°ë³¸ íƒ€ê²Ÿ({target_lang}/{target_lang_code})ìœ¼ë¡œ ì²˜ë¦¬ â†’ {', '.join(unmapped)}")

        # 4) ë³´ê³ ì„œ íŒŒì¼ í—¤ë”
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"translation_review_{timestamp}.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"--- ë²ˆì—­ ê²€ìˆ˜ ë³´ê³ ì„œ (Gemini ë‹¨ì¼ ëª¨ë¸ ê¸°ë°˜ / ê·œì¹™ + Casing ë°˜ì˜) ---\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ ê²€ìˆ˜ í•­ëª©: {total_items}ê°œ\n")
            f.write(f"ì‚¬ìš© ëª¨ë¸: {self.model_name}\n")
            if self.glossary:
                f.write(f"ìš©ì–´ì§‘ ì‚¬ìš©: {len(self.glossary)}ê°œ ì›ì–´ í•­ëª© (ë‹¤ì–¸ì–´ íƒ€ê²Ÿ í¬í•¨)\n")
            sheets_display = ", ".join(selected_sheets_list) if selected_sheets_list else "ì „ì²´"
            f.write(f"ê²€ìˆ˜ ëŒ€ìƒ ì‹œíŠ¸: {sheets_display}\n")
            f.write(f"ë™ì‹œì„± ì œí•œ: {self.max_concurrency}\n")
            f.write(f"ì§§ì€ í…ìŠ¤íŠ¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸: {', '.join(sorted(self.short_text_whitelist))}\n")
            f.write(f"ìš©ì–´ì§‘ ì‚¬ì „ ë¶ˆì¼ì¹˜ ì‹œ LLM ìŠ¤í‚µ: {self.skip_llm_when_glossary_mismatch}\n")
            if sheet_lang_map:
                f.write("ì‹œíŠ¸ë³„ ì–¸ì–´ ì„¤ì •:\n")
                for name, info in sheet_lang_map.items():
                    f.write(f" - {name}: {info['lang']} ({info['code']})\n")
            else:
                f.write(f"(ê¸°ë³¸ íƒ€ê²Ÿ) {target_lang} ({target_lang_code})\n")

        # 5) ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰(ìˆœì„œ ë³´ì¥: gather)
        tasks = [
            self.process_translation_pair_async(
                data_pair=dp,
                source_lang=source_lang,
                default_target_lang=target_lang,
                sheet_lang_map=sheet_lang_map,
                default_target_lang_code=target_lang_code,
            )
            for dp in all_data
        ]
        print(f"\nAPI í˜¸ì¶œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì´ {total_items}ê°œ í•­ëª©, ë™ì‹œì„± {self.max_concurrency})")
        all_results = await asyncio.gather(*tasks)

        # 6) ê²°ê³¼ íŒŒì¼ ê¸°ë¡
        print("âœ“ ë¹„ë™ê¸° ì‘ì—… ì™„ë£Œ. ê²°ê³¼ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ íŒŒì¼ì— ì‘ì„± ì¤‘...")
        with open(output_file, "a", encoding="utf-8") as f:
            for result_content in all_results:
                f.write(result_content)

        print(f"\n{'='*90}")
        print(f"â˜…â˜…â˜… ìµœì¢… ê²€ìˆ˜ ì™„ë£Œ! â˜…â˜…â˜…")
        print(f"ê²°ê³¼ íŒŒì¼: {output_file}")
        print(f"ê²°ê³¼ íŒŒì¼ì€ ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ëœ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"{'='*90}")

    def run(self, *args, **kwargs):
        return asyncio.run(self.main_async(*args, **kwargs))


# ----------------- CLI -----------------
if __name__ == "__main__":
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

    parser = argparse.ArgumentParser(description="ì—‘ì…€ ê¸°ë°˜ ë²ˆì—­ í’ˆì§ˆ ê²€ìˆ˜ ìŠ¤í¬ë¦½íŠ¸ (Gemini ë‹¨ì¼ ëª¨ë¸ ê¸°ë°˜)")
    parser.add_argument("--source_file", required=True, help="ì›ë¬¸ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: original.xlsx)")
    parser.add_argument("--target_file", required=True, help="ë²ˆì—­ë³¸ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: translation.xlsx)")
    parser.add_argument("--range", required=True, help="ê²€ìˆ˜í•  ì…€ ë²”ìœ„ (ì˜ˆ: A2:A100)")
    parser.add_argument("--src_lang", default="English", help="ì›ë¬¸ ì–¸ì–´ (ì˜ˆ: English)")
    parser.add_argument("--tgt_lang", default="Korean", help="ê¸°ë³¸ ë²ˆì—­ ì–¸ì–´ (ì˜ˆ: Korean)")
    parser.add_argument("--src_code", default="en_US", help="ìš©ì–´ì§‘ CSVì˜ ì›ë¬¸ ì–¸ì–´ ì»¬ëŸ¼ëª… (ì˜ˆ: en_US ë˜ëŠ” CSV ì‹¤ì œ í—¤ë”ëª…)")
    parser.add_argument("--tgt_code", default="ko_KR", help="(ê¸°ë³¸) íƒ€ê²Ÿ ì–¸ì–´ ì»¬ëŸ¼ëª… (ì˜ˆ: ko_KR ë˜ëŠ” CSV ì‹¤ì œ í—¤ë”ëª…)")
    parser.add_argument("--glossary", default="glossary.csv", help="ìš©ì–´ì§‘ CSV íŒŒì¼ ê²½ë¡œ (ì„ íƒ ì‚¬í•­)")
    parser.add_argument(
        "--sheet_names",
        help="ê²€ìˆ˜í•  ì‹œíŠ¸ ì´ë¦„ì„ ì‰¼í‘œ(,)ë¡œ ì§€ì • (ì˜ˆ: KR(í•œêµ­),US(ë¯¸êµ­))",
    )
    parser.add_argument(
        "--sheet_langs",
        help="ì‹œíŠ¸ë³„ ì–¸ì–´/ì½”ë“œ (ì˜ˆ: AE(ì•„ëì—ë©”ë¦¬íŠ¸):Arabic:ì•„ëì—ë¯¸ë¦¬íŠ¸)",
    )
    parser.add_argument(
        "--sheet_langs_file",
        help="ì‹œíŠ¸ë³„ ì–¸ì–´/ì½”ë“œ ë§¤í•‘ JSON íŒŒì¼ ê²½ë¡œ (ì˜ˆ: sheet_langs.json)"
    )
    parser.add_argument(
        "--max_concurrency",
        type=int,
        default=10,
        help="ë™ì‹œ í˜¸ì¶œ ì œí•œ (ì„¸ë§ˆí¬ì–´). ê¸°ë³¸ 10, í™˜ê²½ì— ë§ê²Œ 5~15 ê¶Œì¥.",
    )
    parser.add_argument(
        "--whitelist",
        default="",
        help="ì§§ì€ í…ìŠ¤íŠ¸ ìŠ¤í‚µ ì˜ˆì™¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ì½¤ë§ˆ êµ¬ë¶„). ì˜ˆ: OK,ON,OFF,NFC",
    )
    parser.add_argument(
        "--skip_llm_when_glossary_mismatch",
        action="store_true",
        help="ìš©ì–´ì§‘ ì‚¬ì „ ë¶ˆì¼ì¹˜ê°€ ê°ì§€ë˜ë©´ LLM í˜¸ì¶œì„ ìƒëµí•˜ê³  ì‚¬ì „ ê°ì§€ë§Œ ë³´ê³ í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--model",
        default="gemini-2.5-flash",
        help="ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…(ê¸°ë³¸: gemini-2.5-flash)",
    )
    parser.add_argument(
    "--no_backtranslation",
    action="store_true",
    help="ì—­ë²ˆì—­ ë‹¨ê³„ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.",
    )

    args = parser.parse_args()

    try:
        checker = TranslationChecker(
            gemini_api_key=GEMINI_API_KEY,
            model_name=args.model,
            max_concurrency=args.max_concurrency,
            short_text_whitelist=args.whitelist,
            skip_llm_when_glossary_mismatch=args.skip_llm_when_glossary_mismatch,
            no_backtranslation = args.no_backtranslation
        )

        checker.run(
            source_file=args.source_file,
            target_file=args.target_file,
            cell_range=args.range,
            source_lang=args.src_lang,
            target_lang=args.tgt_lang,
            source_lang_code=args.src_code,
            target_lang_code=args.tgt_code,
            glossary_file=args.glossary,
            sheet_names=args.sheet_names,
            sheet_langs=args.sheet_langs,
            sheet_langs_file=args.sheet_langs_file,  # JSON ê²½ë¡œ ì „ë‹¬
        )

    except ValueError as e:
        print(f"\n[FATAL ERROR] ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        print("API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜(.env íŒŒì¼)ì— ì •í™•íˆ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"\n[FATAL ERROR] ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")